{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rdm42\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "<ipython-input-1-4bdac6f14819>:17: DeprecationWarning: This package has been superseded by the `leidenalg` package and will no longer be maintained. Please upgrade to the `leidenalg` package.\n",
      "  import louvain\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import lzma\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "import scipy.sparse as sps\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import networkx as nx\n",
    "import louvain\n",
    "import igraph as ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_degree_summary(Mwd=None, G=None):\n",
    "    \n",
    "    if G is None:\n",
    "        dw = Mwd.sum(axis=1)\n",
    "        print(f\"Number of words: {Mwd.shape[0]}\")\n",
    "        print(f\"Number of documents: {Mwd.shape[1]}\")\n",
    "    else:\n",
    "        dw = pd.Series(dict(G.degree()))\n",
    "        print(f\"Number of words: {len(G)}\")\n",
    "        \n",
    "\n",
    "    # Summary analysis of the network:\n",
    "    gamma = 1 + dw.size/np.log(dw / dw.min()).sum()\n",
    "    print(f\"Network's gamma is {gamma:.2f}\")\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    plt.suptitle(\"Word degree summary\", size=20)\n",
    "\n",
    "    # Words' degrees\n",
    "\n",
    "    n = 6\n",
    "    s = dw.sort_values(ascending=False)\n",
    "    custom_ticks = [min((s.size//n)*l,s.size-1) for l in range(n+1)]\n",
    "    custom_labels = s.index[custom_ticks]\n",
    "\n",
    "    axs[0].set_title(\"Degrees\", size=16)\n",
    "    axs[0].set_ylabel(\"Word count across documents\", size=12)\n",
    "    axs[0].semilogy(s)\n",
    "    axs[0].set_xticks(custom_ticks)\n",
    "    axs[0].set_xticklabels(custom_labels, rotation=30)\n",
    "\n",
    "\n",
    "    # Words degree distribution\n",
    "\n",
    "    k = np.unique(dw)\n",
    "    pk = np.histogram(dw,np.append(k,k[-1]+1))[0]\n",
    "    pk = pk/pk.sum()\n",
    "    # Pk = 1-np.cumsum(pk)\n",
    "    p_ideal = ((gamma-1)/k.min()) * ((k/k.min())**(-gamma))\n",
    "\n",
    "    label = r'$\\gamma$' + f\" = {gamma:.2f}\\n\" + r'$k_{min}$' + f\" = {k.min()}\"\n",
    "    axs[1].loglog(k, pk, 'o')\n",
    "    axs[1].loglog(k, p_ideal,label=label)\n",
    "    axs[1].set_title(\"Distribution\", size=16)\n",
    "    axs[1].set_xlabel(\"k\", size=12)\n",
    "    axs[1].set_ylabel(\"$p_k$\", size=12)\n",
    "    axs[1].legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return gamma, fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_Mwd_matrix(Mwd,words,documents,thresh=2):\n",
    "\n",
    "    # remove elements that are too central, e.g., #covid19\n",
    "    not_wanted = Mwd.sum(axis=1) > Mwd.shape[1]/4\n",
    "    text = \"removing: \" + \", \".join(words[not_wanted])\n",
    "    words = words[~not_wanted]\n",
    "    Mwd = Mwd[~not_wanted,:]\n",
    "\n",
    "    # remove documents and words with fewer than 2 links\n",
    "    while True:\n",
    "        dim_old = Mwd.size\n",
    "        \n",
    "        # remove documents with less than 2 words\n",
    "        wanted = Mwd.sum(axis=0) >= thresh\n",
    "        Mwd = Mwd[:,wanted]\n",
    "        documents = documents[wanted]\n",
    "\n",
    "        # remove words in less than 2 documents\n",
    "        wanted = Mwd.sum(axis=1) >= thresh\n",
    "        text = text + \", \" + \", \".join(words[~wanted])\n",
    "        words = words[wanted]\n",
    "        Mwd = Mwd[wanted,:]\n",
    "        \n",
    "        # exit criterion\n",
    "        if (dim_old == Mwd.size): break\n",
    "\n",
    "    print(text)\n",
    "    return Mwd, words, documents\n",
    "\n",
    "def logg(x):\n",
    "    y = np.log(x)\n",
    "    y[x==0] = 0\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "def probability_matrices(Mwd, equalik = True, tform = False):\n",
    "\n",
    "    if equalik: # documents equally likely\n",
    "        Pwd = Mwd/Mwd.sum(axis=0) #/ Mwd.shape[1]    ## Por qué aquí se divide entre el número de columnas???\n",
    "        \n",
    "    else: # documents proportional to their length\n",
    "        Pwd = Mwd / Mwd.sum()                       ## Aquí la suma por columnas no da 1!!! entonces no es matriz estocástica\n",
    "    # TF-IDF format\n",
    "    \n",
    "    if (tform):\n",
    "        iw = -logg(np.sum(Mwd>0,axis=1).flatten()/Mwd.shape[1])\n",
    "        Pwd = sps.diags(np.array(iw)[0])*Pwd # TF-IDF form\n",
    "        Pwd = Pwd/Pwd.sum() # normalize, treat it as Pwd\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Creo que aquí debe ser .dot ..............\n",
    "    \n",
    "    # words and document matrices\n",
    "    pd = Pwd.sum(axis=0)\n",
    "    Pww = (Pwd/pd).dot(Pwd.T)\n",
    "    \n",
    "    pw = Pwd.sum(axis=1)\n",
    "    Pdd = (Pwd.T/pw).dot(Pwd)\n",
    "    \n",
    "    \n",
    "    # joint words and document matrix - documents first\n",
    "    Paa = sps.hstack((sps.csr_matrix((Pwd.shape[1],Pwd.shape[1])),Pwd.T))\n",
    "    Paa = sps.vstack((Paa,sps.hstack((Pwd,sps.csr_matrix((Pwd.shape[0],Pwd.shape[0]))))))\n",
    "    Paa = Paa/2.0\n",
    "    \n",
    "    return Pwd, Pww, Pdd, Paa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = os.path.join('..','0_data','downloaded_posts')\n",
    "metadata_paths = [os.path.join(download_dir,f) for f in os.listdir(download_dir) if f.endswith('.json.xz')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Short</th>\n",
       "      <th>user</th>\n",
       "      <th>likes</th>\n",
       "      <th>caption</th>\n",
       "      <th>Long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fNO52Nxluy</td>\n",
       "      <td>amenstyle</td>\n",
       "      <td>61</td>\n",
       "      <td>Valentina Vignali looks explosive in a preciou...</td>\n",
       "      <td>2013-10-08_12-40-36_UTC.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lzQl_gRlnD</td>\n",
       "      <td>amenstyle</td>\n",
       "      <td>374</td>\n",
       "      <td>Regram @nimabenati #amenstyle #amen #fashion #...</td>\n",
       "      <td>2014-03-21_10-14-52_UTC.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mrzAb-Rlkt</td>\n",
       "      <td>amenstyle</td>\n",
       "      <td>175</td>\n",
       "      <td>RG @nimabenati #amencouture amenstyle.com</td>\n",
       "      <td>2014-04-12_09-13-03_UTC.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Short       user  likes  \\\n",
       "0  fNO52Nxluy  amenstyle     61   \n",
       "1  lzQl_gRlnD  amenstyle    374   \n",
       "2  mrzAb-Rlkt  amenstyle    175   \n",
       "\n",
       "                                             caption  \\\n",
       "0  Valentina Vignali looks explosive in a preciou...   \n",
       "1  Regram @nimabenati #amenstyle #amen #fashion #...   \n",
       "2          RG @nimabenati #amencouture amenstyle.com   \n",
       "\n",
       "                          Long  \n",
       "0  2013-10-08_12-40-36_UTC.jpg  \n",
       "1  2014-03-21_10-14-52_UTC.jpg  \n",
       "2  2014-04-12_09-13-03_UTC.jpg  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_data = []\n",
    "\n",
    "for i,path in enumerate(metadata_paths):\n",
    "\n",
    "    with lzma.open(path, 'rt') as file:\n",
    "        data_dict = json.load(file)\n",
    "    \n",
    "    short = data_dict['node']['shortcode']\n",
    "    user  = data_dict['node']['owner']['username']\n",
    "    likes = data_dict['node']['edge_media_preview_like']['count']\n",
    "    caption = data_dict['node']['edge_media_to_caption']['edges'][0]['node']['text']\n",
    "    long = f\"{os.path.split(path)[-1].split('.')[0]}.jpg\"\n",
    "\n",
    "    additional_data.append((short,user,likes,caption,long))\n",
    "\n",
    "additional_data = pd.DataFrame(additional_data, columns=['Short','user','likes','caption','Long'])\n",
    "\n",
    "additional_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_path = os.path.join('..','0_data','text_clean.xlsx')\n",
    "s = ' '\n",
    "\n",
    "# text_col = 'clean_caption'\n",
    "# text_col = 'clean_all'\n",
    "text_col = 'clean_autocaption'\n",
    "\n",
    "with pd.ExcelFile(captions_path) as xl:\n",
    "    data = xl.parse(index_col=0) \\\n",
    "             .merge(additional_data, on='Short', how='left', validate='m:1') \\\n",
    "             .drop_duplicates() \\\n",
    "             .reset_index(drop=True)\n",
    "\n",
    "# Inclusion of username in captions\n",
    "for col in ['clean_caption','clean_autocaption','clean_all']:\n",
    "    data[col] = data['user'] + ' ' + data[col]\n",
    "\n",
    "# collection of words\n",
    "words = np.unique([token\n",
    "                   for tokens in data.dropna(subset=[text_col])[text_col].str.split()\n",
    "                   for token in tokens])\n",
    "\n",
    "documents = data[text_col].values\n",
    "\n",
    "# words dictionary\n",
    "words_dict = {wrd:i for i,wrd in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link\n",
    "# post id\n",
    "# description AI\n",
    "# caption\n",
    "# cleaned versions\n",
    "# obj\n",
    "# brand / user\n",
    "# likes\n",
    "\n",
    "# caterina.suitner@unipd.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Occurrence matrix $M_{wd}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# occurrence matrix for words\n",
    "Mwd = csr_matrix((len(words_dict), data.shape[0]), dtype = np.int8).toarray()\n",
    "\n",
    "for j,doc in data[text_col].dropna().items():\n",
    "    for token in doc.split():\n",
    "        Mwd[words_dict[token],j] += 1\n",
    "        \n",
    "        \n",
    "# remove words that are used once or twice\n",
    "select = Mwd.sum(axis=1) > 2\n",
    "Mwd = Mwd[select,:]\n",
    "words = words[select]\n",
    "\n",
    "# remove documents that do not contain words\n",
    "select = Mwd.sum(axis=0) > 0\n",
    "Mwd = Mwd[:,select]\n",
    "documents = documents[select]\n",
    "\n",
    "# Further cleaning and probability matrices computation\n",
    "Mwd, words, documents = clean_Mwd_matrix(Mwd,words,documents,thresh=2)\n",
    "Pwd, Pww, Pdd, Paa = probability_matrices(Mwd, tform=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mwd_df = pd.DataFrame(Mwd,index=words,columns=documents)\n",
    "Pww_df = pd.DataFrame(Pww,index=words,columns=words)\n",
    "\n",
    "gamma, fig, axs = plot_degree_summary(Mwd_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectification rate per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_docs = data.set_index(text_col) \\\n",
    "               .query(\"Objectify==1\") \\\n",
    "               .index.drop_duplicates() \\\n",
    "               .intersection(Mwd_df.columns)\n",
    "\n",
    "\n",
    "# Objectification rate for each word catalogue\n",
    "# obj_rate = times the word is used in an objectifying post / times the word is used\n",
    "obj_rate_cat = (Mwd_df[obj_docs].sum(axis=1) / Mwd_df.sum(axis=1)) \\\n",
    "               .to_frame().rename(columns={0:'obj_rate'}) \\\n",
    "               .reset_index(names='Id')\n",
    "\n",
    "display(obj_rate_cat.head(3))\n",
    "display(obj_rate_cat.describe().T)\n",
    "\n",
    "\n",
    "sns.kdeplot(data=obj_rate_cat, x='obj_rate')\n",
    "sns.rugplot(data=obj_rate_cat, x='obj_rate')\n",
    "\n",
    "### La distribución luce multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = Pww_df.melt(ignore_index=False, var_name='Target', value_name='Weight') \\\n",
    "              .reset_index() \\\n",
    "              .rename(columns={'index':'Source'})\n",
    "\n",
    "links['temp_key'] = links.apply(lambda row: ''.join(sorted([row['Source'],row['Target']])),axis=1)\n",
    "\n",
    "links.groupby('temp_key')['Weight'].std().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = Pww_df.where((np.triu(np.ones(Pww_df.shape))).astype(np.bool_)) \\\n",
    "              .melt(ignore_index=False, var_name='Target', value_name='Weight') \\\n",
    "              .dropna() \\\n",
    "              .query(\"\"\"Weight !=0\"\"\") \\\n",
    "              .reset_index() \\\n",
    "              .rename(columns={'index':'Source'})\n",
    "\n",
    "sns.kdeplot(data=links, x='Weight')\n",
    "sns.rugplot(data=links, x='Weight')\n",
    "links.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = links.Weight.quantile(0.05)\n",
    "data_gephi = links.query(\"Weight >= @tol\").sort_values('Weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(data_gephi, 'Source', 'Target', ['Weight'])\n",
    "dw = pd.Series(dict(G.degree()))\n",
    "_=plot_degree_summary(G=G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gephi.to_csv(f'data/processed/col_{text_col}__tol_{tol}__links.csv',\n",
    "                  encoding='UTF-16',index=False)\n",
    "\n",
    "pd.concat([data_gephi.Source,\n",
    "           data_gephi.Target],\n",
    "          ignore_index=True) \\\n",
    "  .drop_duplicates() \\\n",
    "  .sort_values() \\\n",
    "  .to_frame(name='Id') \\\n",
    "  .eval(\"Label=Id\") \\\n",
    "  .merge(obj_rate_cat,how='left',on='Id',validate='m:1') \\\n",
    "  .to_csv(f'data/processed/col_{text_col}__tol_{tol}__nodes.csv',encoding='UTF-16',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1024*5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
