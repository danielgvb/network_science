{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8faff6e9-a1c4-45fe-9932-b7f678de4274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1eac7cd-b155-486c-ac92-20dc34eca967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>text_deep_clean</th>\n",
       "      <th>objectifies</th>\n",
       "      <th>skin</th>\n",
       "      <th>adj</th>\n",
       "      <th>Sixltr</th>\n",
       "      <th>Dic</th>\n",
       "      <th>percept</th>\n",
       "      <th>see</th>\n",
       "      <th>relativ</th>\n",
       "      <th>...</th>\n",
       "      <th>Authentic</th>\n",
       "      <th>verb</th>\n",
       "      <th>Clout</th>\n",
       "      <th>AllPunc</th>\n",
       "      <th>social</th>\n",
       "      <th>female</th>\n",
       "      <th>Comma</th>\n",
       "      <th>feel</th>\n",
       "      <th>drives</th>\n",
       "      <th>WC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../1_download_data/jacquemus\\2022-05-30_15-43-...</td>\n",
       "      <td>jacquemus sydney hawai i night tom woman long ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.332761</td>\n",
       "      <td>7.69</td>\n",
       "      <td>30.77</td>\n",
       "      <td>84.62</td>\n",
       "      <td>15.38</td>\n",
       "      <td>15.38</td>\n",
       "      <td>30.77</td>\n",
       "      <td>...</td>\n",
       "      <td>94.81</td>\n",
       "      <td>7.69</td>\n",
       "      <td>77.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.69</td>\n",
       "      <td>7.69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../1_download_data/jacquemus\\2020-11-12_17-00-...</td>\n",
       "      <td>jacquemus l annee fw shoot rosa picture woman ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.138254</td>\n",
       "      <td>16.67</td>\n",
       "      <td>13.89</td>\n",
       "      <td>86.11</td>\n",
       "      <td>25.00</td>\n",
       "      <td>22.22</td>\n",
       "      <td>13.89</td>\n",
       "      <td>...</td>\n",
       "      <td>1.79</td>\n",
       "      <td>8.33</td>\n",
       "      <td>79.76</td>\n",
       "      <td>5.56</td>\n",
       "      <td>8.33</td>\n",
       "      <td>8.33</td>\n",
       "      <td>5.56</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../1_download_data/hm\\2023-09-24_18-34-48_UTC\\...</td>\n",
       "      <td>handbag hold close woman wear black dress blac...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>5.41</td>\n",
       "      <td>16.22</td>\n",
       "      <td>86.49</td>\n",
       "      <td>24.32</td>\n",
       "      <td>24.32</td>\n",
       "      <td>10.81</td>\n",
       "      <td>...</td>\n",
       "      <td>4.50</td>\n",
       "      <td>5.41</td>\n",
       "      <td>70.57</td>\n",
       "      <td>2.70</td>\n",
       "      <td>5.41</td>\n",
       "      <td>5.41</td>\n",
       "      <td>2.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../1_download_data/jacquemus\\2022-03-23_16-47-...</td>\n",
       "      <td>jacquemus le sac rond so happy have work iconi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125818</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.74</td>\n",
       "      <td>82.61</td>\n",
       "      <td>26.09</td>\n",
       "      <td>17.39</td>\n",
       "      <td>13.04</td>\n",
       "      <td>...</td>\n",
       "      <td>9.25</td>\n",
       "      <td>4.35</td>\n",
       "      <td>66.81</td>\n",
       "      <td>8.70</td>\n",
       "      <td>8.70</td>\n",
       "      <td>8.70</td>\n",
       "      <td>4.35</td>\n",
       "      <td>8.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../1_download_data/chanelofficial\\2023-12-08_1...</td>\n",
       "      <td>chanelofficial opening chanel metier art show ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.145028</td>\n",
       "      <td>8.33</td>\n",
       "      <td>20.83</td>\n",
       "      <td>79.17</td>\n",
       "      <td>20.83</td>\n",
       "      <td>20.83</td>\n",
       "      <td>12.50</td>\n",
       "      <td>...</td>\n",
       "      <td>7.84</td>\n",
       "      <td>4.17</td>\n",
       "      <td>66.17</td>\n",
       "      <td>4.17</td>\n",
       "      <td>4.17</td>\n",
       "      <td>4.17</td>\n",
       "      <td>4.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            img_path  \\\n",
       "0  ../1_download_data/jacquemus\\2022-05-30_15-43-...   \n",
       "1  ../1_download_data/jacquemus\\2020-11-12_17-00-...   \n",
       "2  ../1_download_data/hm\\2023-09-24_18-34-48_UTC\\...   \n",
       "3  ../1_download_data/jacquemus\\2022-03-23_16-47-...   \n",
       "4  ../1_download_data/chanelofficial\\2023-12-08_1...   \n",
       "\n",
       "                                     text_deep_clean  objectifies      skin  \\\n",
       "0  jacquemus sydney hawai i night tom woman long ...            1  0.332761   \n",
       "1  jacquemus l annee fw shoot rosa picture woman ...            0  0.138254   \n",
       "2  handbag hold close woman wear black dress blac...            1  0.002294   \n",
       "3  jacquemus le sac rond so happy have work iconi...            1  0.125818   \n",
       "4  chanelofficial opening chanel metier art show ...            0  0.145028   \n",
       "\n",
       "     adj  Sixltr    Dic  percept    see  relativ  ...  Authentic  verb  Clout  \\\n",
       "0   7.69   30.77  84.62    15.38  15.38    30.77  ...      94.81  7.69  77.92   \n",
       "1  16.67   13.89  86.11    25.00  22.22    13.89  ...       1.79  8.33  79.76   \n",
       "2   5.41   16.22  86.49    24.32  24.32    10.81  ...       4.50  5.41  70.57   \n",
       "3   0.00   21.74  82.61    26.09  17.39    13.04  ...       9.25  4.35  66.81   \n",
       "4   8.33   20.83  79.17    20.83  20.83    12.50  ...       7.84  4.17  66.17   \n",
       "\n",
       "   AllPunc  social  female  Comma  feel  drives  WC  \n",
       "0     0.00    7.69    7.69   0.00  0.00     0.0  13  \n",
       "1     5.56    8.33    8.33   5.56  2.78     0.0  36  \n",
       "2     2.70    5.41    5.41   2.70  0.00     0.0  37  \n",
       "3     8.70    8.70    8.70   4.35  8.70     0.0  23  \n",
       "4     4.17    4.17    4.17   4.17  0.00     0.0  24  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('../0_data/gold/8_data_model_LWIC_filtered.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be1834d3-51bd-44fa-9c4d-19d288f574ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objectifies</th>\n",
       "      <th>skin</th>\n",
       "      <th>adj</th>\n",
       "      <th>Sixltr</th>\n",
       "      <th>Dic</th>\n",
       "      <th>percept</th>\n",
       "      <th>see</th>\n",
       "      <th>relativ</th>\n",
       "      <th>space</th>\n",
       "      <th>function</th>\n",
       "      <th>...</th>\n",
       "      <th>Authentic</th>\n",
       "      <th>verb</th>\n",
       "      <th>Clout</th>\n",
       "      <th>AllPunc</th>\n",
       "      <th>social</th>\n",
       "      <th>female</th>\n",
       "      <th>Comma</th>\n",
       "      <th>feel</th>\n",
       "      <th>drives</th>\n",
       "      <th>WC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1587.000000</td>\n",
       "      <td>1587.000000</td>\n",
       "      <td>1587.000000</td>\n",
       "      <td>1587.000000</td>\n",
       "      <td>1587.000000</td>\n",
       "      <td>1587.000000</td>\n",
       "      <td>1587.000000</td>\n",
       "      <td>1587.000000</td>\n",
       "      <td>1587.000000</td>\n",
       "      <td>1587.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1587.000000</td>\n",
       "      <td>1587.000000</td>\n",
       "      <td>1587.000000</td>\n",
       "      <td>1587.000000</td>\n",
       "      <td>1587.000000</td>\n",
       "      <td>1587.000000</td>\n",
       "      <td>1587.000000</td>\n",
       "      <td>1587.000000</td>\n",
       "      <td>1587.000000</td>\n",
       "      <td>1587.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.412098</td>\n",
       "      <td>0.228274</td>\n",
       "      <td>8.399647</td>\n",
       "      <td>17.463894</td>\n",
       "      <td>85.361909</td>\n",
       "      <td>19.979912</td>\n",
       "      <td>17.475772</td>\n",
       "      <td>15.456950</td>\n",
       "      <td>14.998488</td>\n",
       "      <td>43.876156</td>\n",
       "      <td>...</td>\n",
       "      <td>18.760353</td>\n",
       "      <td>5.228393</td>\n",
       "      <td>73.593963</td>\n",
       "      <td>6.096925</td>\n",
       "      <td>6.900365</td>\n",
       "      <td>6.083270</td>\n",
       "      <td>4.693132</td>\n",
       "      <td>1.574423</td>\n",
       "      <td>2.447498</td>\n",
       "      <td>30.496534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.492368</td>\n",
       "      <td>0.213313</td>\n",
       "      <td>4.669839</td>\n",
       "      <td>4.874109</td>\n",
       "      <td>5.437738</td>\n",
       "      <td>4.990345</td>\n",
       "      <td>4.453235</td>\n",
       "      <td>5.507293</td>\n",
       "      <td>5.444888</td>\n",
       "      <td>4.316616</td>\n",
       "      <td>...</td>\n",
       "      <td>22.513286</td>\n",
       "      <td>3.118808</td>\n",
       "      <td>9.119266</td>\n",
       "      <td>4.337770</td>\n",
       "      <td>3.206366</td>\n",
       "      <td>2.305879</td>\n",
       "      <td>2.830453</td>\n",
       "      <td>1.936250</td>\n",
       "      <td>2.947700</td>\n",
       "      <td>5.843198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.560000</td>\n",
       "      <td>68.570000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.860000</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>27.270000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065017</td>\n",
       "      <td>5.410000</td>\n",
       "      <td>13.890000</td>\n",
       "      <td>81.480000</td>\n",
       "      <td>16.670000</td>\n",
       "      <td>14.290000</td>\n",
       "      <td>11.760000</td>\n",
       "      <td>11.430000</td>\n",
       "      <td>41.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>3.120000</td>\n",
       "      <td>66.170000</td>\n",
       "      <td>3.120000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.940000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159732</td>\n",
       "      <td>8.330000</td>\n",
       "      <td>17.140000</td>\n",
       "      <td>85.190000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>17.390000</td>\n",
       "      <td>15.150000</td>\n",
       "      <td>14.290000</td>\n",
       "      <td>43.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.870000</td>\n",
       "      <td>4.550000</td>\n",
       "      <td>72.170000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.060000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.336554</td>\n",
       "      <td>11.430000</td>\n",
       "      <td>20.830000</td>\n",
       "      <td>88.890000</td>\n",
       "      <td>23.080000</td>\n",
       "      <td>20.590000</td>\n",
       "      <td>18.750000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>46.880000</td>\n",
       "      <td>...</td>\n",
       "      <td>26.550000</td>\n",
       "      <td>7.410000</td>\n",
       "      <td>79.760000</td>\n",
       "      <td>8.570000</td>\n",
       "      <td>8.570000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>3.120000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>22.580000</td>\n",
       "      <td>41.670000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>40.620000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>61.540000</td>\n",
       "      <td>...</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>27.590000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>15.150000</td>\n",
       "      <td>26.670000</td>\n",
       "      <td>13.330000</td>\n",
       "      <td>26.670000</td>\n",
       "      <td>38.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       objectifies         skin          adj       Sixltr          Dic  \\\n",
       "count  1587.000000  1587.000000  1587.000000  1587.000000  1587.000000   \n",
       "mean      0.412098     0.228274     8.399647    17.463894    85.361909   \n",
       "std       0.492368     0.213313     4.669839     4.874109     5.437738   \n",
       "min       0.000000     0.000000     0.000000     5.560000    68.570000   \n",
       "25%       0.000000     0.065017     5.410000    13.890000    81.480000   \n",
       "50%       0.000000     0.159732     8.330000    17.140000    85.190000   \n",
       "75%       1.000000     0.336554    11.430000    20.830000    88.890000   \n",
       "max       1.000000     0.999600    22.580000    41.670000   100.000000   \n",
       "\n",
       "           percept          see      relativ        space     function  ...  \\\n",
       "count  1587.000000  1587.000000  1587.000000  1587.000000  1587.000000  ...   \n",
       "mean     19.979912    17.475772    15.456950    14.998488    43.876156  ...   \n",
       "std       4.990345     4.453235     5.507293     5.444888     4.316616  ...   \n",
       "min       5.710000     5.000000     2.860000     2.780000    27.270000  ...   \n",
       "25%      16.670000    14.290000    11.760000    11.430000    41.180000  ...   \n",
       "50%      20.000000    17.390000    15.150000    14.290000    43.750000  ...   \n",
       "75%      23.080000    20.590000    18.750000    18.180000    46.880000  ...   \n",
       "max      40.620000    35.000000    40.000000    40.000000    61.540000  ...   \n",
       "\n",
       "         Authentic         verb        Clout      AllPunc       social  \\\n",
       "count  1587.000000  1587.000000  1587.000000  1587.000000  1587.000000   \n",
       "mean     18.760353     5.228393    73.593963     6.096925     6.900365   \n",
       "std      22.513286     3.118808     9.119266     4.337770     3.206366   \n",
       "min       1.000000     0.000000    50.000000     0.000000     2.700000   \n",
       "25%       2.400000     3.120000    66.170000     3.120000     4.350000   \n",
       "50%       8.870000     4.550000    72.170000     5.000000     6.060000   \n",
       "75%      26.550000     7.410000    79.760000     8.570000     8.570000   \n",
       "max      99.000000    20.000000    99.000000    27.590000    30.000000   \n",
       "\n",
       "            female        Comma         feel       drives           WC  \n",
       "count  1587.000000  1587.000000  1587.000000  1587.000000  1587.000000  \n",
       "mean      6.083270     4.693132     1.574423     2.447498    30.496534  \n",
       "std       2.305879     2.830453     1.936250     2.947700     5.843198  \n",
       "min       2.700000     0.000000     0.000000     0.000000    10.000000  \n",
       "25%       4.000000     2.940000     0.000000     0.000000    27.000000  \n",
       "50%       5.710000     4.000000     0.000000     2.700000    32.000000  \n",
       "75%       8.000000     5.880000     3.120000     3.700000    35.000000  \n",
       "max      15.150000    26.670000    13.330000    26.670000    38.000000  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf8db833-1481-4472-9d8f-bb7005c79015",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = df.select_dtypes(include=['number']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b3cd18b-324b-45ff-8a7c-e681c3839042",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[numeric_columns].drop(columns = 'objectifies').copy()\n",
    "y = df['objectifies']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9d71b8b-fe10-481f-868f-4fc125d31ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\anaconda3\\envs\\ds_master\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.59958071278826\n",
      "KNN Accuracy: 0.6289308176100629\n",
      "SVM Accuracy: 0.5911949685534591\n",
      "Decision Tree Accuracy: 0.5366876310272537\n",
      "Random Forest Accuracy: 0.6184486373165619\n",
      "Gradient Boosting Accuracy: 0.59958071278826\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg_pred = logreg.predict(X_test)\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(X_test)\n",
    "\n",
    "# Support Vector Machine\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "svm_pred = svm.predict(X_test)\n",
    "\n",
    "# Decision Tree\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train, y_train)\n",
    "dtree_pred = dtree.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "rforest = RandomForestClassifier()\n",
    "rforest.fit(X_train, y_train)\n",
    "rforest_pred = rforest.predict(X_test)\n",
    "\n",
    "# Gradient Boosting\n",
    "gboost = GradientBoostingClassifier()\n",
    "gboost.fit(X_train, y_train)\n",
    "gboost_pred = gboost.predict(X_test)\n",
    "\n",
    "# Example of evaluating the models using accuracy\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, logreg_pred))\n",
    "print(\"KNN Accuracy:\", accuracy_score(y_test, knn_pred))\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, svm_pred))\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, dtree_pred))\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rforest_pred))\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, gboost_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e62f38-1787-4b2d-b40c-7ffbf0305ce0",
   "metadata": {},
   "source": [
    "## Use Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8592e580-d5f6-44ae-9063-eb06c83a310e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeabcbe2-950f-4791-b517-21223137e676",
   "metadata": {},
   "source": [
    "Download the Word2Vec Embeddings and save them locally. Not in this folder because github does not support large files in the free trial\n",
    "GoogleNews-vectors-negative300.bin\n",
    "path to download = https://www.kaggle.com/datasets/leadbest/googlenewsvectorsnegative300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c5d7274-6757-42f0-92a3-7f7428bedc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('../../../Embeddings/GoogleNews-vectors-negative300.bin/GoogleNews-vectors-negative300.bin', binary=True) # have embeddings in pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6bd8710-0728-4faf-89a8-dd4a8e1b36cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg_vector(sentence, model):\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    else:\n",
    "        return np.mean(word_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7cb2cbc-c80b-4792-adcd-1bbabc3c3658",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence_vector'] = df['text_deep_clean'].apply(lambda x: sentence_to_avg_vector(x, model))\n",
    "# Assuming you want to use these vectors in a machine learning model\n",
    "X_embeddings = np.array(df['sentence_vector'].tolist())  # Feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bc620f0-c4ac-4760-bca3-a09b77ebab2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.28784180e-02,  4.12375703e-02, -8.36736523e-03, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.30000000e+01],\n",
       "       [ 1.17519209e-02, -1.64370332e-02,  9.43523925e-03, ...,\n",
       "         2.78000000e+00,  0.00000000e+00,  3.60000000e+01],\n",
       "       [ 3.10849268e-02,  3.15718213e-03, -4.59345020e-02, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  3.70000000e+01],\n",
       "       ...,\n",
       "       [-1.88683402e-02, -4.65901708e-03, -2.67808698e-02, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  2.90000000e+01],\n",
       "       [-2.45157885e-03,  7.34268203e-02, -1.33702597e-02, ...,\n",
       "         2.86000000e+00,  0.00000000e+00,  3.50000000e+01],\n",
       "       [ 3.72042656e-02, -1.88064575e-03, -3.41339111e-02, ...,\n",
       "         3.12000000e+00,  6.25000000e+00,  3.20000000e+01]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_combined = np.concatenate((X_embeddings, X), axis=1)\n",
    "X_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364a6e9f-b543-460c-81f5-a6b2824e05e8",
   "metadata": {},
   "source": [
    "# Classificator with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b5c583e-3d55-43c7-acb7-ace9f70cfc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[numeric_columns].drop(columns = 'objectifies').copy()\n",
    "y = df['objectifies']\n",
    "model  = KeyedVectors.load_word2vec_format('../../../Embeddings/GoogleNews-vectors-negative300.bin/GoogleNews-vectors-negative300.bin', binary=True) # have embeddings in pc\n",
    "def sentence_to_avg_vector(sentence, model):\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    else:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "df['sentence_vector'] = df['text_deep_clean'].apply(lambda x: sentence_to_avg_vector(x, model))\n",
    "# Assuming you want to use these vectors in a machine learning model\n",
    "X_embeddings = np.array(df['sentence_vector'].tolist())  # Feature matrix\n",
    "X_combined = np.concatenate((X_embeddings, X), axis=1)\n",
    "X_combined\n",
    "\n",
    "X0 = X_combined\n",
    "Y = df['objectifies']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X0, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01b8c19e-83cf-49f2-ba9c-f8bd0381dbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\anaconda3\\envs\\ds_master\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.59958071278826\n",
      "KNN Accuracy: 0.6310272536687631\n",
      "SVM Accuracy: 0.5849056603773585\n",
      "Decision Tree Accuracy: 0.6121593291404612\n",
      "Random Forest Accuracy: 0.6750524109014675\n",
      "Gradient Boosting Accuracy: 0.6981132075471698\n"
     ]
    }
   ],
   "source": [
    "'''X = df[['skin', 'face']]\n",
    "y = df['objectifies']\n",
    "model  = KeyedVectors.load_word2vec_format('../../../Embeddings/GoogleNews-vectors-negative300.bin/GoogleNews-vectors-negative300.bin', binary=True) # have embeddings in pc\n",
    "def sentence_to_avg_vector(sentence, model):\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    else:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "df['sentence_vector'] = df['text_deep_clean'].apply(lambda x: sentence_to_avg_vector(x, model))\n",
    "# Assuming you want to use these vectors in a machine learning model\n",
    "X_embeddings = np.array(df['sentence_vector'].tolist())  # Feature matrix\n",
    "X_combined = np.concatenate((X_embeddings, X), axis=1)\n",
    "X_combined\n",
    "\n",
    "X0 = X_combined\n",
    "Y = df['objectifies']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X0, Y, test_size=0.3, random_state=42)'''\n",
    "\n",
    "\n",
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg_pred = logreg.predict(X_test)\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(X_test)\n",
    "\n",
    "# Support Vector Machine\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "svm_pred = svm.predict(X_test)\n",
    "\n",
    "# Decision Tree\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train, y_train)\n",
    "dtree_pred = dtree.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "rforest = RandomForestClassifier()\n",
    "rforest.fit(X_train, y_train)\n",
    "rforest_pred = rforest.predict(X_test)\n",
    "\n",
    "# Gradient Boosting\n",
    "gboost = GradientBoostingClassifier()\n",
    "gboost.fit(X_train, y_train)\n",
    "gboost_pred = gboost.predict(X_test)\n",
    "\n",
    "# Example of evaluating the models using accuracy\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, logreg_pred))\n",
    "print(\"KNN Accuracy:\", accuracy_score(y_test, knn_pred))\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, svm_pred))\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, dtree_pred))\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rforest_pred))\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, gboost_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce4d04bf-b28f-4be5-8a7d-088647460021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4120982986767486"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85360579-73fd-495f-b4ec-47c67a967f63",
   "metadata": {},
   "source": [
    "See also F1-score to evaluate best model because dataset is unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1440e10b-c65e-4e0c-b467-acc7124b54ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.4365781710914454\n",
      "KNN F1 Score: 0.5111111111111112\n",
      "SVM F1 Score: 0.0\n",
      "Decision Tree F1 Score: 0.5292620865139949\n",
      "Random Forest F1 Score: 0.5259938837920489\n",
      "Gradient Boosting F1 Score: 0.5909090909090909\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculate F1 scores\n",
    "f1_logreg = f1_score(y_test, logreg_pred)\n",
    "f1_knn = f1_score(y_test, knn_pred)\n",
    "f1_svm = f1_score(y_test, svm_pred)\n",
    "f1_dtree = f1_score(y_test, dtree_pred)\n",
    "f1_rforest = f1_score(y_test, rforest_pred)\n",
    "f1_gboost = f1_score(y_test, gboost_pred)\n",
    "\n",
    "# Print F1 scores\n",
    "print(\"Logistic Regression F1 Score:\", f1_logreg)\n",
    "print(\"KNN F1 Score:\", f1_knn)\n",
    "print(\"SVM F1 Score:\", f1_svm)\n",
    "print(\"Decision Tree F1 Score:\", f1_dtree)\n",
    "print(\"Random Forest F1 Score:\", f1_rforest)\n",
    "print(\"Gradient Boosting F1 Score:\", f1_gboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ba1f36-5761-42e8-9db5-941c0666ec28",
   "metadata": {},
   "source": [
    "Having the best model we can automatically label all the other data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19d6a7a-eba9-44cc-9ab8-7d92f7f3a78d",
   "metadata": {},
   "source": [
    "# See feature importance on objectification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3d3ad8b-9491-4c7a-906d-1bd38757ffff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.6876310272536688\n",
      "XGBoost F1: 0.5802816901408451\n",
      "Feature: word_embedding_82, Importance: 0.024113457649946213\n",
      "Feature: word_embedding_13, Importance: 0.017740368843078613\n",
      "Feature: word_embedding_198, Importance: 0.01563941314816475\n",
      "Feature: word_embedding_114, Importance: 0.011732355691492558\n",
      "Feature: word_embedding_284, Importance: 0.011593455448746681\n",
      "Feature: word_embedding_191, Importance: 0.011519678868353367\n",
      "Feature: word_embedding_267, Importance: 0.011336554773151875\n",
      "Feature: word_embedding_133, Importance: 0.010250293649733067\n",
      "Feature: word_embedding_32, Importance: 0.010205714963376522\n",
      "Feature: word_embedding_107, Importance: 0.010112588293850422\n",
      "Feature: word_embedding_48, Importance: 0.009962476789951324\n",
      "Feature: word_embedding_38, Importance: 0.009949743747711182\n",
      "Feature: word_embedding_287, Importance: 0.009908782318234444\n",
      "Feature: word_embedding_94, Importance: 0.009761146269738674\n",
      "Feature: word_embedding_92, Importance: 0.009344803169369698\n",
      "Feature: word_embedding_66, Importance: 0.008890087716281414\n",
      "Feature: word_embedding_99, Importance: 0.008749234490096569\n",
      "Feature: word_embedding_256, Importance: 0.008646203204989433\n",
      "Feature: word_embedding_206, Importance: 0.008557816036045551\n",
      "Feature: word_embedding_115, Importance: 0.00790362898260355\n",
      "Feature: word_embedding_60, Importance: 0.007805387489497662\n",
      "Feature: word_embedding_0, Importance: 0.007802695035934448\n",
      "Feature: word_embedding_227, Importance: 0.007681621704250574\n",
      "Feature: word_embedding_23, Importance: 0.007492197677493095\n",
      "Feature: word_embedding_203, Importance: 0.007419498171657324\n",
      "Feature: word_embedding_120, Importance: 0.007138883229345083\n",
      "Feature: word_embedding_71, Importance: 0.007118972949683666\n",
      "Feature: word_embedding_261, Importance: 0.0071096401661634445\n",
      "Feature: word_embedding_230, Importance: 0.0065908231772482395\n",
      "Feature: word_embedding_280, Importance: 0.006363936234265566\n",
      "Feature: word_embedding_95, Importance: 0.006352735683321953\n",
      "Feature: word_embedding_161, Importance: 0.006339516956359148\n",
      "Feature: word_embedding_222, Importance: 0.006254879292100668\n",
      "Feature: word_embedding_184, Importance: 0.006244704592972994\n",
      "Feature: word_embedding_142, Importance: 0.006157010793685913\n",
      "Feature: word_embedding_54, Importance: 0.00612621009349823\n",
      "Feature: word_embedding_148, Importance: 0.006072452291846275\n",
      "Feature: word_embedding_53, Importance: 0.005926067009568214\n",
      "Feature: word_embedding_105, Importance: 0.005908370018005371\n",
      "Feature: word_embedding_52, Importance: 0.005882752127945423\n",
      "Feature: word_embedding_21, Importance: 0.005728608462959528\n",
      "Feature: word_embedding_137, Importance: 0.005529208108782768\n",
      "Feature: word_embedding_180, Importance: 0.005459863692522049\n",
      "Feature: word_embedding_236, Importance: 0.0052832942456007\n",
      "Feature: word_embedding_111, Importance: 0.0052437870763242245\n",
      "Feature: word_embedding_17, Importance: 0.005225018132477999\n",
      "Feature: word_embedding_5, Importance: 0.005190849304199219\n",
      "Feature: word_embedding_124, Importance: 0.005069987382739782\n",
      "Feature: word_embedding_28, Importance: 0.005041935946792364\n",
      "Feature: word_embedding_128, Importance: 0.005008410662412643\n",
      "Feature: word_embedding_192, Importance: 0.005004713777452707\n",
      "Feature: word_embedding_146, Importance: 0.004957036580890417\n",
      "Feature: word_embedding_98, Importance: 0.004863596521317959\n",
      "Feature: word_embedding_215, Importance: 0.004819032270461321\n",
      "Feature: word_embedding_218, Importance: 0.00480214087292552\n",
      "Feature: word_embedding_59, Importance: 0.004800320137292147\n",
      "Feature: word_embedding_276, Importance: 0.004760229028761387\n",
      "Feature: word_embedding_112, Importance: 0.004704602528363466\n",
      "Feature: word_embedding_274, Importance: 0.00469239242374897\n",
      "Feature: skin, Importance: 0.0046603260561823845\n",
      "Feature: word_embedding_187, Importance: 0.004656144417822361\n",
      "Feature: word_embedding_204, Importance: 0.004637440200895071\n",
      "Feature: word_embedding_45, Importance: 0.004595315549522638\n",
      "Feature: word_embedding_34, Importance: 0.004564570728689432\n",
      "Feature: word_embedding_129, Importance: 0.004538084380328655\n",
      "Feature: word_embedding_10, Importance: 0.004496918525546789\n",
      "Feature: word_embedding_208, Importance: 0.004428145941346884\n",
      "Feature: word_embedding_56, Importance: 0.004360268358141184\n",
      "Feature: word_embedding_253, Importance: 0.004307129420340061\n",
      "Feature: word_embedding_126, Importance: 0.004304179456084967\n",
      "Feature: word_embedding_135, Importance: 0.004260025452822447\n",
      "Feature: word_embedding_213, Importance: 0.004252300132066011\n",
      "Feature: word_embedding_101, Importance: 0.004239958245307207\n",
      "Feature: word_embedding_69, Importance: 0.004213930573314428\n",
      "Feature: word_embedding_58, Importance: 0.004211634397506714\n",
      "Feature: word_embedding_251, Importance: 0.004207890015095472\n",
      "Feature: word_embedding_140, Importance: 0.0041510178707540035\n",
      "Feature: word_embedding_35, Importance: 0.004148961044847965\n",
      "Feature: word_embedding_125, Importance: 0.0041479128412902355\n",
      "Feature: word_embedding_297, Importance: 0.004049774259328842\n",
      "Feature: word_embedding_196, Importance: 0.003988140728324652\n",
      "Feature: word_embedding_76, Importance: 0.003964873496443033\n",
      "Feature: word_embedding_123, Importance: 0.003949345555156469\n",
      "Feature: word_embedding_212, Importance: 0.0039300755597651005\n",
      "Feature: word_embedding_74, Importance: 0.003883518511429429\n",
      "Feature: word_embedding_159, Importance: 0.003778706304728985\n",
      "Feature: word_embedding_24, Importance: 0.003775200108066201\n",
      "Feature: word_embedding_152, Importance: 0.0037718764506280422\n",
      "Feature: word_embedding_290, Importance: 0.0037590537685900927\n",
      "Feature: word_embedding_55, Importance: 0.0037086328957229853\n",
      "Feature: word_embedding_22, Importance: 0.003648273879662156\n",
      "Feature: word_embedding_156, Importance: 0.0036424328573048115\n",
      "Feature: word_embedding_41, Importance: 0.0036419264506548643\n",
      "Feature: word_embedding_295, Importance: 0.0036331925075501204\n",
      "Feature: word_embedding_97, Importance: 0.0035727503709495068\n",
      "Feature: word_embedding_157, Importance: 0.003555272938683629\n",
      "Feature: word_embedding_226, Importance: 0.003550979308784008\n",
      "Feature: word_embedding_219, Importance: 0.003533043898642063\n",
      "Feature: word_embedding_164, Importance: 0.0035307162906974554\n",
      "Feature: word_embedding_78, Importance: 0.0035292983520776033\n",
      "Feature: word_embedding_138, Importance: 0.003459102474153042\n",
      "Feature: word_embedding_289, Importance: 0.003448542207479477\n",
      "Feature: word_embedding_67, Importance: 0.0034436406567692757\n",
      "Feature: word_embedding_88, Importance: 0.0034427279606461525\n",
      "Feature: word_embedding_64, Importance: 0.0034086231607943773\n",
      "Feature: word_embedding_265, Importance: 0.0033825053833425045\n",
      "Feature: word_embedding_250, Importance: 0.0033692007418721914\n",
      "Feature: word_embedding_216, Importance: 0.003350610611960292\n",
      "Feature: word_embedding_172, Importance: 0.003336191875860095\n",
      "Feature: word_embedding_170, Importance: 0.003332671243697405\n",
      "Feature: word_embedding_189, Importance: 0.0033109497744590044\n",
      "Feature: word_embedding_176, Importance: 0.0032929042354226112\n",
      "Feature: word_embedding_288, Importance: 0.003283577272668481\n",
      "Feature: word_embedding_268, Importance: 0.003215567907318473\n",
      "Feature: word_embedding_231, Importance: 0.003180482890456915\n",
      "Feature: word_embedding_109, Importance: 0.003162340261042118\n",
      "Feature: word_embedding_153, Importance: 0.003152579767629504\n",
      "Feature: word_embedding_96, Importance: 0.003097076201811433\n",
      "Feature: word_embedding_224, Importance: 0.00309509364888072\n",
      "Feature: word_embedding_39, Importance: 0.0030841261614114046\n",
      "Feature: word_embedding_197, Importance: 0.0030669912230223417\n",
      "Feature: word_embedding_193, Importance: 0.0030641998164355755\n",
      "Feature: word_embedding_121, Importance: 0.0030605262145400047\n",
      "Feature: word_embedding_29, Importance: 0.0030346871353685856\n",
      "Feature: word_embedding_144, Importance: 0.0030326370615512133\n",
      "Feature: word_embedding_103, Importance: 0.003002116223797202\n",
      "Feature: word_embedding_255, Importance: 0.0029980360995978117\n",
      "Feature: word_embedding_209, Importance: 0.002967321779578924\n",
      "Feature: word_embedding_70, Importance: 0.002939724363386631\n",
      "Feature: word_embedding_68, Importance: 0.002893551019951701\n",
      "Feature: word_embedding_270, Importance: 0.0028095992747694254\n",
      "Feature: word_embedding_116, Importance: 0.002774282591417432\n",
      "Feature: word_embedding_132, Importance: 0.002768391277641058\n",
      "Feature: word_embedding_186, Importance: 0.0027558112051337957\n",
      "Feature: word_embedding_217, Importance: 0.0027275318279862404\n",
      "Feature: word_embedding_229, Importance: 0.0027265867684036493\n",
      "Feature: word_embedding_81, Importance: 0.002721671247854829\n",
      "Feature: word_embedding_257, Importance: 0.0027023828588426113\n",
      "Feature: word_embedding_150, Importance: 0.0027007735334336758\n",
      "Feature: word_embedding_102, Importance: 0.0026903541292995214\n",
      "Feature: word_embedding_51, Importance: 0.0026900845114141703\n",
      "Feature: word_embedding_225, Importance: 0.0026681420858949423\n",
      "Feature: word_embedding_27, Importance: 0.0026545985601842403\n",
      "Feature: word_embedding_269, Importance: 0.0026463225949555635\n",
      "Feature: word_embedding_232, Importance: 0.0026307017542421818\n",
      "Feature: word_embedding_117, Importance: 0.002615635981783271\n",
      "Feature: word_embedding_131, Importance: 0.0026066319551318884\n",
      "Feature: word_embedding_293, Importance: 0.0025800166185945272\n",
      "Feature: word_embedding_16, Importance: 0.002549646655097604\n",
      "Feature: word_embedding_15, Importance: 0.002538879169151187\n",
      "Feature: word_embedding_185, Importance: 0.002538342960178852\n",
      "Feature: word_embedding_89, Importance: 0.0025372332893311977\n",
      "Feature: word_embedding_239, Importance: 0.0025329049676656723\n",
      "Feature: word_embedding_237, Importance: 0.0025165043771266937\n",
      "Feature: word_embedding_91, Importance: 0.0025041354820132256\n",
      "Feature: word_embedding_294, Importance: 0.002459453884512186\n",
      "Feature: word_embedding_149, Importance: 0.0024474705569446087\n",
      "Feature: word_embedding_271, Importance: 0.0024226605892181396\n",
      "Feature: word_embedding_235, Importance: 0.0024207285605371\n",
      "Feature: word_embedding_46, Importance: 0.0024028215557336807\n",
      "Feature: face, Importance: 0.002370563568547368\n",
      "Feature: word_embedding_158, Importance: 0.002366415923461318\n",
      "Feature: word_embedding_275, Importance: 0.0023587101604789495\n",
      "Feature: word_embedding_118, Importance: 0.0023263278417289257\n",
      "Feature: word_embedding_143, Importance: 0.0023155936505645514\n",
      "Feature: word_embedding_18, Importance: 0.0023143840953707695\n",
      "Feature: word_embedding_281, Importance: 0.0022926293313503265\n",
      "Feature: word_embedding_87, Importance: 0.0022609219886362553\n",
      "Feature: word_embedding_145, Importance: 0.0022123069502413273\n",
      "Feature: word_embedding_6, Importance: 0.0021644672378897667\n",
      "Feature: word_embedding_119, Importance: 0.002163289813324809\n",
      "Feature: word_embedding_136, Importance: 0.002163040917366743\n",
      "Feature: word_embedding_173, Importance: 0.0021250408608466387\n",
      "Feature: word_embedding_44, Importance: 0.0020776349119842052\n",
      "Feature: word_embedding_181, Importance: 0.001999997068196535\n",
      "Feature: word_embedding_245, Importance: 0.0019813268445432186\n",
      "Feature: word_embedding_211, Importance: 0.0019513749284669757\n",
      "Feature: word_embedding_188, Importance: 0.0019497271860018373\n",
      "Feature: word_embedding_179, Importance: 0.0019423493649810553\n",
      "Feature: word_embedding_108, Importance: 0.0019349288195371628\n",
      "Feature: word_embedding_183, Importance: 0.001929883612319827\n",
      "Feature: word_embedding_42, Importance: 0.0019136477494612336\n",
      "Feature: word_embedding_240, Importance: 0.001896945759654045\n",
      "Feature: word_embedding_86, Importance: 0.0018903629388660192\n",
      "Feature: word_embedding_30, Importance: 0.0018893532687798142\n",
      "Feature: word_embedding_291, Importance: 0.0018883203156292439\n",
      "Feature: word_embedding_282, Importance: 0.0018334691412746906\n",
      "Feature: word_embedding_7, Importance: 0.001746420981362462\n",
      "Feature: word_embedding_258, Importance: 0.001727594411931932\n",
      "Feature: word_embedding_155, Importance: 0.0017157522961497307\n",
      "Feature: word_embedding_223, Importance: 0.0017010904848575592\n",
      "Feature: word_embedding_277, Importance: 0.0016854526475071907\n",
      "Feature: word_embedding_122, Importance: 0.0016788368811830878\n",
      "Feature: word_embedding_1, Importance: 0.0016596638597548008\n",
      "Feature: word_embedding_104, Importance: 0.001659027999266982\n",
      "Feature: word_embedding_200, Importance: 0.0016417365986853838\n",
      "Feature: word_embedding_246, Importance: 0.001622590934857726\n",
      "Feature: word_embedding_26, Importance: 0.0016117775812745094\n",
      "Feature: word_embedding_168, Importance: 0.0016083032824099064\n",
      "Feature: word_embedding_75, Importance: 0.0016017224406823516\n",
      "Feature: word_embedding_233, Importance: 0.001598845119588077\n",
      "Feature: word_embedding_93, Importance: 0.00153335090726614\n",
      "Feature: word_embedding_298, Importance: 0.001498061465099454\n",
      "Feature: word_embedding_2, Importance: 0.0014855900080874562\n",
      "Feature: word_embedding_33, Importance: 0.001478009857237339\n",
      "Feature: word_embedding_248, Importance: 0.001472644042223692\n",
      "Feature: word_embedding_205, Importance: 0.0014563187723979354\n",
      "Feature: word_embedding_139, Importance: 0.0014381245709955692\n",
      "Feature: word_embedding_72, Importance: 0.0014326933305710554\n",
      "Feature: word_embedding_57, Importance: 0.0014234541449695826\n",
      "Feature: word_embedding_262, Importance: 0.0014190592337399721\n",
      "Feature: word_embedding_162, Importance: 0.0014181995065882802\n",
      "Feature: word_embedding_73, Importance: 0.0014096756931394339\n",
      "Feature: word_embedding_228, Importance: 0.0013915176969021559\n",
      "Feature: word_embedding_106, Importance: 0.0013900287449359894\n",
      "Feature: word_embedding_177, Importance: 0.0013646276202052832\n",
      "Feature: word_embedding_83, Importance: 0.0013492879224941134\n",
      "Feature: word_embedding_151, Importance: 0.0013478954788297415\n",
      "Feature: word_embedding_202, Importance: 0.0012792457127943635\n",
      "Feature: word_embedding_3, Importance: 0.001274225185625255\n",
      "Feature: word_embedding_134, Importance: 0.0012315994827076793\n",
      "Feature: word_embedding_4, Importance: 0.001228672219440341\n",
      "Feature: word_embedding_266, Importance: 0.001227252185344696\n",
      "Feature: word_embedding_263, Importance: 0.0012218109332025051\n",
      "Feature: word_embedding_260, Importance: 0.0011672250693663955\n",
      "Feature: word_embedding_8, Importance: 0.0011637879069894552\n",
      "Feature: word_embedding_25, Importance: 0.0011491442564874887\n",
      "Feature: word_embedding_175, Importance: 0.0011433539912104607\n",
      "Feature: word_embedding_166, Importance: 0.0011186697520315647\n",
      "Feature: word_embedding_141, Importance: 0.0011108379112556577\n",
      "Feature: word_embedding_182, Importance: 0.0010767207713797688\n",
      "Feature: word_embedding_201, Importance: 0.0010587346041575074\n",
      "Feature: word_embedding_130, Importance: 0.0010380013845860958\n",
      "Feature: word_embedding_299, Importance: 0.0010165576823055744\n",
      "Feature: word_embedding_160, Importance: 0.001007768209092319\n",
      "Feature: word_embedding_36, Importance: 0.0009875696850940585\n",
      "Feature: word_embedding_190, Importance: 0.0009839634876698256\n",
      "Feature: word_embedding_100, Importance: 0.0008958075195550919\n",
      "Feature: word_embedding_264, Importance: 0.0008941663545556366\n",
      "Feature: word_embedding_11, Importance: 0.0008618548745289445\n",
      "Feature: word_embedding_63, Importance: 0.0008458825759589672\n",
      "Feature: word_embedding_110, Importance: 0.0008453907794319093\n",
      "Feature: word_embedding_43, Importance: 0.0008386514964513481\n",
      "Feature: word_embedding_278, Importance: 0.0008301023626700044\n",
      "Feature: word_embedding_259, Importance: 0.0008268653764389455\n",
      "Feature: word_embedding_234, Importance: 0.0008230980020016432\n",
      "Feature: word_embedding_252, Importance: 0.0008220763993449509\n",
      "Feature: word_embedding_14, Importance: 0.0008076110389083624\n",
      "Feature: word_embedding_85, Importance: 0.0008014636696316302\n",
      "Feature: word_embedding_90, Importance: 0.0007869597757235169\n",
      "Feature: word_embedding_285, Importance: 0.000780523638240993\n",
      "Feature: word_embedding_199, Importance: 0.0007786596543155611\n",
      "Feature: word_embedding_154, Importance: 0.0007743206806480885\n",
      "Feature: word_embedding_165, Importance: 0.0007724643219262362\n",
      "Feature: word_embedding_272, Importance: 0.0007514647440984845\n",
      "Feature: word_embedding_169, Importance: 0.000703466881532222\n",
      "Feature: word_embedding_61, Importance: 0.0006548129022121429\n",
      "Feature: word_embedding_279, Importance: 0.0006401435821317136\n",
      "Feature: word_embedding_37, Importance: 0.0006179142510518432\n",
      "Feature: word_embedding_174, Importance: 0.0006115302676334977\n",
      "Feature: word_embedding_249, Importance: 0.0006052633398212492\n",
      "Feature: word_embedding_84, Importance: 0.0005885937716811895\n",
      "Feature: word_embedding_163, Importance: 0.0005731027922593057\n",
      "Feature: word_embedding_221, Importance: 0.0005422864924184978\n",
      "Feature: word_embedding_19, Importance: 0.0005200184532441199\n",
      "Feature: word_embedding_283, Importance: 0.0005063149728812277\n",
      "Feature: word_embedding_80, Importance: 0.0004897643229924142\n",
      "Feature: word_embedding_286, Importance: 0.0004667868197429925\n",
      "Feature: word_embedding_77, Importance: 0.0004589622258208692\n",
      "Feature: word_embedding_50, Importance: 0.0004479499766603112\n",
      "Feature: word_embedding_20, Importance: 0.0004424619837664068\n",
      "Feature: word_embedding_9, Importance: 0.00043453756370581686\n",
      "Feature: word_embedding_207, Importance: 0.0003751192707568407\n",
      "Feature: word_embedding_147, Importance: 0.00036723644006997347\n",
      "Feature: word_embedding_79, Importance: 0.0003575437585823238\n",
      "Feature: word_embedding_210, Importance: 0.0003555954899638891\n",
      "Feature: word_embedding_127, Importance: 0.00034848981886170805\n",
      "Feature: word_embedding_241, Importance: 0.00029043058748357\n",
      "Feature: word_embedding_244, Importance: 0.0002897586382459849\n",
      "Feature: word_embedding_47, Importance: 0.0002690640394575894\n",
      "Feature: word_embedding_62, Importance: 0.00022512873692903668\n",
      "Feature: word_embedding_65, Importance: 0.00019807952048722655\n",
      "Feature: word_embedding_214, Importance: 0.00015719668590463698\n",
      "Feature: word_embedding_296, Importance: 0.0001553734764456749\n",
      "Feature: word_embedding_292, Importance: 0.0001503953681094572\n",
      "Feature: word_embedding_254, Importance: 0.0001226829772349447\n",
      "Feature: word_embedding_238, Importance: 0.00011236355931032449\n",
      "Feature: word_embedding_247, Importance: 9.772915655048564e-05\n",
      "Feature: word_embedding_12, Importance: 6.751091859769076e-05\n",
      "Feature: word_embedding_167, Importance: 2.682403828657698e-05\n",
      "Feature: word_embedding_31, Importance: 0.0\n",
      "Feature: word_embedding_40, Importance: 0.0\n",
      "Feature: word_embedding_49, Importance: 0.0\n",
      "Feature: word_embedding_113, Importance: 0.0\n",
      "Feature: word_embedding_171, Importance: 0.0\n",
      "Feature: word_embedding_178, Importance: 0.0\n",
      "Feature: word_embedding_194, Importance: 0.0\n",
      "Feature: word_embedding_195, Importance: 0.0\n",
      "Feature: word_embedding_220, Importance: 0.0\n",
      "Feature: word_embedding_242, Importance: 0.0\n",
      "Feature: word_embedding_243, Importance: 0.0\n",
      "Feature: word_embedding_273, Importance: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Your existing code for data preparation...\n",
    "\n",
    "# Train an XGBoost model\n",
    "xgboost_model = XGBClassifier()\n",
    "xgboost_model.fit(X_train, Y_train)\n",
    "xgboost_pred = xgboost_model.predict(X_test)\n",
    "\n",
    "# Evaluate the XGBoost model\n",
    "print(\"XGBoost Accuracy:\", accuracy_score(Y_test, xgboost_pred))\n",
    "print(\"XGBoost F1:\", f1_score(Y_test, xgboost_pred))\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importances = xgboost_model.feature_importances_\n",
    "\n",
    "# Combine word embedding feature names with 'skin' and 'face'\n",
    "word_embedding_feature_names = ['word_embedding_' + str(i) for i in range(X_embeddings.shape[1])]\n",
    "other_feature_names = ['skin', 'face']\n",
    "feature_names = word_embedding_feature_names + other_feature_names\n",
    "\n",
    "# Pair feature names with their importances\n",
    "features = dict(zip(feature_names, feature_importances))\n",
    "sorted_features = sorted(features.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Displaying feature importances\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"Feature: {feature}, Importance: {importance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6416c873-491b-48da-84c3-78e77c96fcf5",
   "metadata": {},
   "source": [
    "For model interpretability, especially in the context of feature importance from a model like XGBoost, we are limited to understanding the importance of the entire word embedding as a feature, not its individual dimensions. Each dimension of the embedding contributes to the overall meaning of the word in a way that's intertwined with all other dimensions, making it difficult to assign specific meaning or importance to individual dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abb7af3-1630-4f75-971d-b282e8adf2e3",
   "metadata": {},
   "source": [
    "But seeing that embeddings contribute to the classification task, shows that the content of the shows 8 embeddings that are more relevant that the fact of face or not in a picture for example, showing great promissing in a greater dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b998d4bc-d412-47ae-8a58-41c19fc5b1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: thigh, Similarity: 0.6789047718048096\n",
      "Word: legs, Similarity: 0.6698594093322754\n",
      "Word: ankle, Similarity: 0.651005744934082\n",
      "Word: shin, Similarity: 0.6278131604194641\n",
      "Word: forearm, Similarity: 0.627385675907135\n",
      "Word: knee, Similarity: 0.613674521446228\n",
      "Word: collar_bone, Similarity: 0.6121348738670349\n",
      "Word: puncturing_lung, Similarity: 0.6104021668434143\n",
      "Word: collarbone, Similarity: 0.6058264970779419\n",
      "Word: fibula, Similarity: 0.6012815833091736\n"
     ]
    }
   ],
   "source": [
    "# Choose a target word\n",
    "target_word = \"leg\"  # replace with your target word\n",
    "\n",
    "# Find most similar words\n",
    "most_similar_words = model.most_similar(target_word, topn=10)\n",
    "\n",
    "for word, similarity in most_similar_words:\n",
    "    print(f\"Word: {word}, Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64db3094-614b-460b-9c39-2438adc900ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample DataFrame\n",
    "# df = pd.DataFrame({'text_deep_clean': ['your sentences here', 'more sentences']})\n",
    "\n",
    "# Tokenize each sentence and get unique words\n",
    "unique_words = set()\n",
    "for sentence in df['text_deep_clean']:\n",
    "    words = word_tokenize(sentence)\n",
    "    unique_words.update(words)\n",
    "\n",
    "# Convert set to list if you need a list\n",
    "unique_word_list = list(unique_words)\n",
    "words_to_compare = unique_word_list\n",
    "# unique_word_list now contains all unique words from your sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85c50726-0117-41a2-ad99-cd5a165d9d0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cosine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m model:\n\u001b[0;32m     19\u001b[0m         word_vector \u001b[38;5;241m=\u001b[39m model[word]\n\u001b[1;32m---> 20\u001b[0m         similarity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m cosine(average_vector, word_vector)\n\u001b[0;32m     21\u001b[0m         similarities[word] \u001b[38;5;241m=\u001b[39m similarity\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Sort and display the words based on similarity\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cosine' is not defined"
     ]
    }
   ],
   "source": [
    "# First, let's identify the top 10 features that are word embeddings\n",
    "top_word_embedding_indices = [int(feature.split('_')[-1]) for feature, importance in sorted_features[:10] if 'word_embedding_' in feature]\n",
    "# Calculate the average vector across the top features\n",
    "\n",
    "average_vector = np.zeros(model.vector_size)  # Initialize a zero vector of the same size as the word vectors\n",
    "\n",
    "# Calculate the average for each top feature\n",
    "for index in top_word_embedding_indices:\n",
    "    # Summing up all the vectors corresponding to this dimension\n",
    "    average_vector += np.mean(X_embeddings[:, index])\n",
    "\n",
    "# Normalize the average vector\n",
    "average_vector /= len(top_word_embedding_indices)\n",
    "\n",
    "# Compute cosine similarity for each word in your list\n",
    "similarities = {}\n",
    "for word in words_to_compare:\n",
    "    if word in model:\n",
    "        word_vector = model[word]\n",
    "        similarity = 1 - cosine(average_vector, word_vector)\n",
    "        similarities[word] = similarity\n",
    "\n",
    "# Sort and display the words based on similarity\n",
    "sorted_words = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "for word, similarity in sorted_words:\n",
    "    print(f\"Word: {word}, Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685d5fbb-fc25-4323-a17e-5555bf6db8e2",
   "metadata": {},
   "source": [
    "This is a shallow approxmation of what we could see as words that objectify the body of women as described by the BLIP model captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2247893f-d758-4b60-a0c3-1116291bad39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
